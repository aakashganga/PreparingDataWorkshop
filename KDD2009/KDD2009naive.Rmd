---
title: "KDD2009naive"
author: "Win-Vector LLC"
date: "November 9, 2015"
output: html_document
---

[KDD2009 naive example](http://www.sigkdd.org/kdd-cup-2009-customer-relationship-prediction).  Winners had hold-out AUC of 0.7611 on churn.   See [here](https://github.com/WinVector/zmPDSwR/tree/master/KDD2009) for more details.

```{r kddexlibs, tidy=FALSE}
print(date())
#load some libraries
# http://www.win-vector.com/blog/2014/08/vtreat-designing-a-package-for-variable-treatment/
library('vtreat')
# devtools::install_github("WinVector/WVPlots")
library('WVPlots')

library('parallel')
library('gbm')
library('ggplot2')


# load the data as in the book
# change this path to match your directory structure
dir = './' 
debug = FALSE


d = read.table(paste(dir,'orange_small_train.data.gz',sep=''),
                header=T,sep='\t',na.strings=c('NA',''), 
               stringsAsFactors=FALSE)
churn = read.table(paste(dir,'orange_small_train_churn.labels.txt',sep=''),
                    header=F,sep='\t')
d$churn = churn$V1
appetency = read.table(paste(dir,'orange_small_train_appetency.labels.txt',sep=''),
                        header=F,sep='\t')
d$appetency = appetency$V1
upselling = read.table(paste(dir,'orange_small_train_upselling.labels.txt',sep=''),
                        header=F,sep='\t')
d$upselling = upselling$V1
set.seed(729375)
d$rgroup = runif(dim(d)[[1]])
dTrainM = subset(d,rgroup<=0.9)  # set for building models
dTest = subset(d,rgroup>0.9) # set for evaluation
if(debug) {
  dTrainM <- dTrainM[sample.int(nrow(dTrainM),1000),]
  dTest <- dTest[sample.int(nrow(dTest),1000),]
}
rm(list=c('d','churn','appetency','upselling','dir'))
outcomes = c('churn','appetency','upselling')
nonvars <- c(outcomes,'rgroup')
vars = setdiff(colnames(dTrainM),
                nonvars)
yName = 'churn'
yTarget = 1
print(date())
```

```{r kddprint, tidy=FALSE}
print(summary(dTrainM))
```


The above data will not work with many machine learning algorithms (for example logistic regression, or random forests) due to the many NA entries, and the presence of large categoricals.  In addition many R implementations only accept a limited number of independent variable types.  We will perform the minimum amount of "fixing" to make this dataset usable, however the transformations are complicated and in no sense optimal.

What we will do is:
 
 * Change logical to numeric.
 * Change numeric NA to 0.
 * Temporarily change all factors to character types (to make sure there are no unused levels).
 * Change all character NA and levels not seen during training to "" (blank).
 * Then change all character types to factor (as this is what gbm accepts, though we are using glm).
 * Limit down only input variabels that have more than one value and categoricals that have no more than 200 values.
 
This "minimal" set of transformations is a bit involved, is not very faithful to the original data, and still may miss corner cases.  This is why this step is worth automating.
 

```{r kdddesign, tidy=FALSE}
print(date())
# naively transform data
cleanColumn <- function(c,targetSet) {
  if(is.logical(c)) {
    c <- as.numeric(c)
  }
  if(is.numeric(c)) {
    c[is.na(c)] <- 0
  }
  if(is.factor(c)) {
    c <- as.character(c)
  }
  if(is.character(c)) {
    c[is.na(c)] <- ''
    if(!is.null(targetSet)) {
      c[!(c %in% targetSet)] <- ''
    }
  }
  c
}
treatedTrainM <- dTrainM
treatedTest <- dTest
for(cn in vars) {
  treatedTrainM[[cn]] <- cleanColumn(dTrainM[[cn]],NULL)
  treatedTest[[cn]] <- cleanColumn(dTest[[cn]],unique(treatedTrainM[[cn]]))
  if(is.character(treatedTrainM[[cn]])) {
     levs <- sort(unique(c('',treatedTrainM[[cn]])))
     treatedTrainM[[cn]] <- factor(treatedTrainM[[cn]],levs)
     treatedTest[[cn]] <- factor(treatedTest[[cn]],levs)
  }
}
treatedTrainM[[yName]] = treatedTrainM[[yName]]==yTarget
treatedTest[[yName]] = treatedTest[[yName]]==yTarget
canUse <- vapply(vars,
                 function(cn) { (length(unique(treatedTrainM[[cn]]))>1) &&
                     ( (!is.factor(treatedTrainM[[cn]])) ||
                         (length(unique(treatedTrainM[[cn]]))<=200) ) },
                 logical(1))
selvars <- vars[canUse]
print(date())
```





```{r kddmodels, tidy=FALSE}
print(date())
print(selvars)

# prepare plotting frames
treatedTrainP = treatedTrainM[, yName, drop=FALSE]
treatedTestP = treatedTest[, yName, drop=FALSE]


formulaS = paste(yName,paste(selvars,collapse=' + '),sep=' ~ ')
print("*****************************")
print(date())
mname <- 'glmPred'  # gbm crashes with: Error in object$var.levels[[i]] : subscript out of bounds
print(paste(mname,length(selvars)))
modelglms = glm(as.formula(formulaS),
                data=treatedTrainM,
                family=binomial(link='logit')
)
#print(summary(modelglms))
treatedTrainP[[mname]] = predict(modelglms,newdata=treatedTrainM,type='response')

# can only score test rows, that don't expose novel levels (something been trying hard to avoid)
good <- logical(nrow(treatedTest))
good <- TRUE
for(v in vars) {
  if(is.factor(treatedTrainM[[v]])) {
     good <- good & (treatedTest[[v]] %in% unique(treatedTrainM[[v]]))
  }
}
print(summary(good))
treatedTestP[[mname]] <- NA
treatedTestP[[mname]][good] = predict(modelglms,
                                      newdata=treatedTest[good,,drop=FALSE],
                                      type='response')
treatedTestP[[mname]][is.na(treatedTestP[[mname]])] <- mean(treatedTrainP[[yName]])

t1 = paste(mname,'trainingM data')
print(DoubleDensityPlot(treatedTrainP, mname, yName, 
                        title=t1))
print(ROCPlot(treatedTrainP, mname, yName, 
              title=t1))

t2 = paste(mname,'test data')
print(DoubleDensityPlot(treatedTestP, mname, yName, 
                        title=t2))
print(ROCPlot(treatedTestP, mname, yName, 
              title=t2))
print(date())
```
