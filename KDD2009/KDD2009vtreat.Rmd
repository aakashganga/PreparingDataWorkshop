
[KDD2009 example](http://www.sigkdd.org/kdd-cup-2009-customer-relationship-prediction).  Winners had hold-out AUC of 0.7611 on churn.   See [here](https://github.com/WinVector/zmPDSwR/tree/master/KDD2009) for more details.

```{r kddexlibs, tidy=FALSE}
#load some libraries
# http://www.win-vector.com/blog/2014/08/vtreat-designing-a-package-for-variable-treatment/
library('vtreat')
# devtools::install_github("WinVector/WVPlots")
library('WVPlots')

library('parallel')
library('gbm')
library('ggplot2')


# load the data as in the book
# change this path to match your directory structure
dir = '~/Documents/work/PracticalDataScienceWithR/zmPDSwR/KDD2009/' 

d = read.table(paste(dir,'orange_small_train.data.gz',sep=''),
                header=T,sep='\t',na.strings=c('NA',''), 
               stringsAsFactors=FALSE)
churn = read.table(paste(dir,'orange_small_train_churn.labels.txt',sep=''),
                    header=F,sep='\t')
d$churn = churn$V1
appetency = read.table(paste(dir,'orange_small_train_appetency.labels.txt',sep=''),
                        header=F,sep='\t')
d$appetency = appetency$V1
upselling = read.table(paste(dir,'orange_small_train_upselling.labels.txt',sep=''),
                        header=F,sep='\t')
d$upselling = upselling$V1
set.seed(729375)
d$rgroup = runif(dim(d)[[1]])
dTrainM = subset(d,rgroup<=0.5)  # set for building models
dTrainC = subset(d,(rgroup>0.5) & (rgroup<=0.9)) # set for impact coding
dTest = subset(d,rgroup>0.9) # set for evaluation
debug = FALSE
if(debug) {
  dTrainM <- dTrainM[sample.int(nrow(dTrainM),100),]
  dTrainC <- dTrainC[sample.int(nrow(dTrainC),100),]
  dTest <- dTest[sample.int(nrow(dTest),100),]
}
rm(list=c('d','churn','appetency','upselling','dir'))
outcomes = c('churn','appetency','upselling')
nonvars <- c(outcomes,'rgroup')
vars = setdiff(colnames(dTrainM),
                nonvars)
yName = 'churn'
yTarget = 1
```

```{r kddprint, tidy=FALSE}
print(summary(dTrainM))
```



```{r kdddesign, tidy=FALSE}
# build data treatments

set.seed(239525)

cl <- c()
if(!debug) {
  ncore <- parallel::detectCores()
  cl <- parallel::makeCluster(ncore)
}

# build treatments on just the coding data
treatmentsC = designTreatmentsC(dTrainC,
    vars,yName,yTarget,
    smFactor=2.0, 
    parallelCluster=cl,
    verbose=TRUE)

kddSig = 0.05

addOtherTargets = FALSE

treatedTrainM = prepare(treatmentsC,
                        dTrainM,
                        pruneSig=kddSig, 
                        parallelCluster=cl)
selvars = setdiff(colnames(treatedTrainM),nonvars)
treatedTrainM[[yName]] = treatedTrainM[[yName]]==yTarget

treatedTest = prepare(treatmentsC,
                      dTest,
                      pruneSig=kddSig, 
                      parallelCluster=cl)
treatedTest[[yName]] = treatedTest[[yName]]==yTarget


if(!is.null(cl)) {
    parallel::stopCluster(cl)
    cl = NULL
}
```





```{r kddmodels, tidy=FALSE}
# Run other models (with proper coding/training separation).
#
# This gets us back to AUC 0.72

print(selvars)

# prepare plotting frames
treatedTrainP = treatedTrainM[, yName, drop=FALSE]
treatedTestP = treatedTest[, yName, drop=FALSE]


formulaS = paste(yName,paste(selvars,collapse=' + '),sep=' ~ ')
for(mname in c('gbmPred','glmPred')) {
  print("*****************************")
  print(date())
  print(paste(mname,length(selvars)))
  if(mname=='gbmPred') {
    modelGBMs = gbm(as.formula(formulaS),
                    data=treatedTrainM,
                    distribution='bernoulli',
                    n.trees=1000,
                    interaction.depth=3,
                    keep.data=FALSE,
                    cv.folds=5)
    #print(modelGBMs)
    #print(summary(modelGBMs))
    nTrees = gbm.perf(modelGBMs)
    treatedTrainP[[mname]] = predict(modelGBMs,newdata=treatedTrainM,type='response',
                                     n.trees=nTrees) 
    treatedTestP[[mname]] = predict(modelGBMs,newdata=treatedTest,type='response',
                                    n.trees=nTrees)
  } else {
    modelglms = glm(as.formula(formulaS),
                    data=treatedTrainM,
                    family=binomial(link='logit')
    )
    #print(summary(modelglms))
    treatedTrainP[[mname]] = predict(modelglms,newdata=treatedTrainM,type='response')
    treatedTestP[[mname]] = predict(modelglms,newdata=treatedTest,type='response')
  }
  
  t1 = paste(mname,'trainingM data')
  print(DoubleDensityPlot(treatedTrainP, mname, yName, 
                          title=t1))
  print(ROCPlot(treatedTrainP, mname, yName, 
                title=t1))
  
  t2 = paste(mname,'test data')
  print(DoubleDensityPlot(treatedTestP, mname, yName, 
                          title=t2))
  print(ROCPlot(treatedTestP, mname, yName, 
                title=t2))
  print(date())
  print("*****************************")
}

```
