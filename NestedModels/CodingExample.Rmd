---
title: "CodingExample"
author: "Win-Vector LLC"
date: "September 30, 2015"
output: html_document
---


```{r, echo=FALSE, results='hide', warning=FALSE}
source('model.R')
source('mkModel.R')
# devtools::install_github("WinVector/WVPlots")
library("WVPlots")

runAll = TRUE
cl = NULL

set.seed(232567)
vplan <- list(designVar('x1',10),
              designVar('x2',10),
              designVar('x3',10),
              designVar('x4',10),
              designVar('x5',10),
              designVar('x6',10),
              designVar('x7',10),
              designVar('x8',10),
              designVar('x9',10),
              designVar('x10',10),
              designNoiseVar('n1',500),
              designNoiseVar('n2',500),
              designNoiseVar('n3',500),
              designNoiseVar('n4',500),
              designNoiseVar('n5',500),
              designNoiseVar('n6',500),
              designNoiseVar('n7',500),
              designNoiseVar('n8',500),
              designNoiseVar('n9',500),
              designNoiseVar('n10',500),
              designNoiseVar('n11',500),
              designNoiseVar('n12',500),
              designNoiseVar('n13',500),
              designNoiseVar('n14',500),
              designNoiseVar('n15',500),
              designNoiseVar('n16',500),
              designNoiseVar('n17',500),
              designNoiseVar('n18',500),
              designNoiseVar('n19',500),
              designNoiseVar('n20',500),
              designNoiseVar('n21',500),
              designNoiseVar('n22',500),
              designNoiseVar('n23',500),
              designNoiseVar('n24',500),
              designNoiseVar('n25',500),
              designNoiseVar('n26',500),
              designNoiseVar('n27',500),
              designNoiseVar('n28',500),
              designNoiseVar('n29',500),
              designNoiseVar('n30',500))
yName <- 'y'

dTrain <- generateExample(vplan,2000)
vars <- setdiff(colnames(dTrain),yName)
dTest <- generateExample(vplan,10000)


errorRate <- function(pred,truth) {
  sum((pred>=0.5)!=truth)/length(truth)
}
```

```{r, echo=FALSE, results='hide', warning=FALSE}
cl <- NULL
if(runAll) {
  cl <- parallel::makeCluster(parallel::detectCores())
}
```



Show a standard Bayes model encoding of categorical variables (essentially what vtreat does).
Notice the variables fool the glm built on top of this data into thinking it has a perfect model
on data.  This is because the Bayes encoded columns are hiding the consumption of a very
larger number of degrees of freedom.  They are memorizing things about the training data
that are not relations that will hold into production.


```{r}
bSigma <- 0
print(paste('Bayes model, sigma=',bSigma))
bCoder <- trainBayesCoder(dTrain,yName,vars,bSigma)
dTrainB <- bCoder$codeFrame(dTrain)
dTestB <- bCoder$codeFrame(dTest)
varsB <- setdiff(colnames(dTrainB),yName)
formulaB <- paste(yName,paste(varsB,collapse=' + '),sep=' ~ ')
modelB <- glm(formulaB,data=dTrainB,family=binomial(link='logit'))
dTrainB$pred <- predict(modelB,newdata=dTrainB,type='response')
print(WVPlots::ROCPlot(dTrainB,'pred',yName,
                       paste('Bayes model train, sigma=',bSigma)))
dTestB$pred <- predict(modelB,newdata=dTestB,type='response')
print(errorRate(dTestB$pred,dTestB[[yName]]))
print(WVPlots::ROCPlot(dTestB,'pred',yName,
                       paste('Bayes model test, sigma=',bSigma)))
```


Misha Bilenko presented a related technique ["learning from counts"](http://blogs.technet.com/b/machinelearning/archive/2015/02/17/big-learning-made-easy-with-counts.aspx)
that encodes the large categorical as four columns such that the
Bayes model is a simple function of the columns.  It works a bit
better, but exhibits the same over-fitting issue.

```{r}
cSigma <- 0
print(paste('count model, sigma=',cSigma))
cCoder <- trainCountCoder(dTrain,yName,vars,cSigma)
dTrainC <- cCoder$codeFrame(dTrain)
dTestC <- cCoder$codeFrame(dTest)
varsC <- setdiff(colnames(dTrainC),yName)
formulaC <- paste(yName,paste(varsC,collapse=' + '),sep=' ~ ')
modelC <- glm(formulaC,data=dTrainC,family=binomial(link='logit'))
dTrainC$pred <- predict(modelC,newdata=dTrainC,type='response')
print(WVPlots::ROCPlot(dTrainC,'pred',yName,
                       paste('count model train, sigma=',cSigma)))
dTestC$pred <- predict(modelC,newdata=dTestC,type='response')
print(errorRate(dTestC$pred,dTestC[[yName]]))
print(WVPlots::ROCPlot(dTestC,'pred',yName,
                       paste('count model test, sigma=',cSigma)))
```


One extremely statistically efficient way to reserve a disjoint set of
data for building the variable encodings is a Jackknife technique.
It works very well.





```{r}
print('Bayes model, jackknifed')
bCoder <- trainBayesCoder(dTrain,yName,vars,0)
# dTrainB <- bCoder$codeFrame(dTrain) # naive coding, fails
dTrainB <- jackknifeBayesCode(dTrain,yName,vars)
dTestB <- bCoder$codeFrame(dTest)
varsB <- setdiff(colnames(dTrainB),yName)
formulaB <- paste(yName,paste(varsB,collapse=' + '),sep=' ~ ')
modelB <- glm(formulaB,data=dTrainB,family=binomial(link='logit'))
dTrainB$pred <- predict(modelB,newdata=dTrainB,type='response')
print(WVPlots::ROCPlot(dTrainB,'pred',yName,
                       'Bayes model train, jackknifed'))
dTestB$pred <- predict(modelB,newdata=dTestB,type='response')
print(errorRate(dTestB$pred,dTestB[[yName]]))
print(WVPlots::ROCPlot(dTestB,'pred',yName,
                       'Bayes model test, jackknifed'))
```

```{r}
print('count model, jackknifed')
cCoder <- trainCountCoder(dTrain,yName,vars,0)
# dTrainC <- cCoder$codeFrame(dTrain) # naive coding, fails
dTrainC <- jackknifeCountCode(dTrain,yName,vars)
dTestC <- cCoder$codeFrame(dTest)
varsC <- setdiff(colnames(dTrainC),yName)
formulaC <- paste(yName,paste(varsC,collapse=' + '),sep=' ~ ')
modelC <- glm(formulaC,data=dTrainC,family=binomial(link='logit'))
dTrainC$pred <- predict(modelC,newdata=dTrainC,type='response')
print(WVPlots::ROCPlot(dTrainC,'pred',yName,
                       'count model train, jackknifed'))
dTestC$pred <- predict(modelC,newdata=dTestC,type='response')
print(errorRate(dTestC$pred,dTestC[[yName]]))
print(WVPlots::ROCPlot(dTestC,'pred',yName,
                       'count model test, jackknifed'))
```

Our advised best practice for vtreat is to reserve one set for variable encoding, one for model
training, and a final one for test.  This also works well if you have enough data.


```{r}
print("vtreat split model")
isCal <- runif(nrow(dTrain))<0.5
dTrainC <- dTrain[isCal,]
dTrainT <- dTrain[!isCal,]
treatments <- vtreat::designTreatmentsC(dTrainC,vars,yName,TRUE,
                                        rareSig=0.3,
                                        smFactor=5.0,
                                        minFraction=2.0,
                                        verbose=FALSE,
                                        parallelCluster=cl)
print(treatments$scoreFrame)
dTrainV <- vtreat::prepare(treatments,dTrainT,pruneSig=0.05,
                           parallelCluster=cl)
dTestV <- vtreat::prepare(treatments,dTest,pruneSig=0.05,
                          parallelCluster=cl)
varsV <- setdiff(colnames(dTrainV),yName)
formulaV <- paste(yName,paste(varsV,collapse=' + '),sep=' ~ ')
modelV <- glm(formulaV,data=dTrainV,family=binomial(link='logit'))
dTestV$pred <- predict(modelV,newdata=dTestV,type='response')
print(errorRate(dTestV$pred,dTestV[[yName]]))
print(WVPlots::ROCPlot(dTestV,'pred',yName,
                       paste('vtreat split model test')))
print(WVPlots::DoubleDensityPlot(dTestV,'pred',yName,
                       paste('vtreat split model test')))
```



We can use all our data for both 
encoding construction and training prune out rare levels before they
are allowed into the transformed variables.

```{r}
print("vtreat model")
treatments <- vtreat::designTreatmentsC(dTrain,vars,yName,TRUE,
                                        rareSig=0.3,
                                        smFactor=5.0,
                                        minFraction=2.0,
                                        verbose=FALSE,
                                        parallelCluster=cl)
print(treatments$scoreFrame)
dTrainV <- vtreat::prepare(treatments,dTrain,pruneSig=0.05,
                           parallelCluster=cl)
dTestV <- vtreat::prepare(treatments,dTest,pruneSig=0.05,
                          parallelCluster=cl)
varsV <- setdiff(colnames(dTrainV),yName)
formulaV <- paste(yName,paste(varsV,collapse=' + '),sep=' ~ ')
modelV <- glm(formulaV,data=dTrainV,family=binomial(link='logit'))
dTrainV$pred <- predict(modelV,newdata=dTrainV,type='response')
print(WVPlots::ROCPlot(dTrainV,'pred',yName,
                       paste('vtreat model train')))
dTestV$pred <- predict(modelV,newdata=dTestV,type='response')
print(errorRate(dTestV$pred,dTestV[[yName]]))
print(WVPlots::ROCPlot(dTestV,'pred',yName,
                       paste('vtreat model test')))
print(WVPlots::DoubleDensityPlot(dTestV,'pred',yName,
                       paste('vtreat model test')))
```


Also Misha Bilenko introduced an [application of differential privacy](http://conf.dato.com/speakers/dr-misha-bilenko/)
to protect from over-fitting effects.  It is a bit subtle, but
also works well.  For more on this see [our series on
differential privacy](http://www.win-vector.com/blog/2015/11/our-differential-privacy-mini-series/).


```{r, echo=FALSE, results='hide', warning=FALSE}
mkWorker1 <- function() {
  bindToEnv(environment(),
            yName,
            dTrain,
            vars,
            dTest,
            errorRate,
            rlaplace,
            noiseCount,
            conditionalCounts,
            listLookup,
            bayesCode,
            trainCoder,
            codeFrame,
            trainBayesCoder,
            countCode,
            trainCountCoder)
  function(sigma) {
    cCoder <- trainCountCoder(dTrain,yName,vars,sigma)
    dTrainC <- cCoder$codeFrame(dTrain)
    dTestC <- cCoder$codeFrame(dTest)
    varsC <- setdiff(colnames(dTrainC),yName)
    formulaC <- paste(yName,paste(varsC,collapse=' + '),sep=' ~ ')
    modelC <- glm(formulaC,data=dTrainC,family=binomial(link='logit'))
    dTestC$pred <- predict(modelC,newdata=dTestC,type='response')
    scoreC <- errorRate(dTestC$pred,dTestC[[yName]])
    bCoder <- trainBayesCoder(dTrain,yName,vars,sigma)
    dTrainB <- bCoder$codeFrame(dTrain)
    dTestB <- bCoder$codeFrame(dTest)
    varsB <- setdiff(colnames(dTrainB),yName)
    formulaB <- paste(yName,paste(varsB,collapse=' + '),sep=' ~ ')
    modelB <- glm(formulaB,data=dTrainB,family=binomial(link='logit'))
    dTestB$pred <- predict(modelB,newdata=dTestB,type='response')
    scoreB <- errorRate(dTestB$pred,dTestB[[yName]])
    list(scoreC=scoreC,scoreB=scoreB,sigma=sigma)
  }
}

cSigmaBest = 0
bSigmaBest = 0

if(runAll) {
  results <- parallel::parLapplyLB(cl,(seq_len(201)-1),mkWorker1())
  
  bestC = Inf
  bestB = Inf
  for(res in results) {
    sigma <- res$sigma
    scoreC <- res$scoreC
    scoreB <- res$scoreB
    if(scoreC<bestC) {
      bestC <- scoreC
      cSigmaBest <- sigma
    }
    if(scoreB<bestB) {
      bestB <- scoreB
      bSigmaBest <- sigma
    }
  }
}
```

```{r}
cSigma <- cSigmaBest
print(paste('count model, sigma=',cSigma))
cCoder <- trainCountCoder(dTrain,yName,vars,cSigma)
dTrainC <- cCoder$codeFrame(dTrain)
dTestC <- cCoder$codeFrame(dTest)
varsC <- setdiff(colnames(dTrainC),yName)
formulaC <- paste(yName,paste(varsC,collapse=' + '),sep=' ~ ')
modelC <- glm(formulaC,data=dTrainC,family=binomial(link='logit'))
dTrainC$pred <- predict(modelC,newdata=dTrainC,type='response')
print(WVPlots::ROCPlot(dTrainC,'pred',yName,
                       paste('count model train, sigma=',cSigma)))
dTestC$pred <- predict(modelC,newdata=dTestC,type='response')
print(errorRate(dTestC$pred,dTestC[[yName]]))
print(WVPlots::ROCPlot(dTestC,'pred',yName,
                       paste('count model test, sigma=',cSigma)))
```



```{r}
bSigma <- bSigmaBest
print(paste('Bayes model, sigma=',bSigma))
bCoder <- trainBayesCoder(dTrain,yName,vars,bSigma)
dTrainB <- bCoder$codeFrame(dTrain)
dTestB <- bCoder$codeFrame(dTest)
varsB <- setdiff(colnames(dTrainB),yName)
formulaB <- paste(yName,paste(varsB,collapse=' + '),sep=' ~ ')
modelB <- glm(formulaB,data=dTrainB,family=binomial(link='logit'))
dTrainB$pred <- predict(modelB,newdata=dTrainB,type='response')
print(WVPlots::ROCPlot(dTrainB,'pred',yName,
                       paste('Bayes model train, sigma=',bSigma)))
dTestB$pred <- predict(modelB,newdata=dTestB,type='response')
print(errorRate(dTestB$pred,dTestB[[yName]]))
print(WVPlots::ROCPlot(dTestB,'pred',yName,
                       paste('Bayes model test, sigma=',bSigma)))
```



```{r}
if(!is.null(cl)) {
  parallel::stopCluster(cl)
  cl <- NULL
}
```
