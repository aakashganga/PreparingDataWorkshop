---
title: "Y-Aware Principal Components Analysis"
author: "Nina Zumel"
date: "April 19, 2016"
output: html_document
---

In this note, we show an example of using [`vtreat`](https://github.com/WinVector/vtreat)'s `prepare(scale=TRUE)` option for _y_-aware principal components analysis in [R](https://cran.r-project.org/). Y-aware PCA lets you analyze problem structure and potentially reduce problem dimensionality while explicitly accounting for the relationships between the dependent and independent variables, something that standard ("_x_-only") PCA does not do.


```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width=7, fig.heigh=7, 
                      warning=FALSE, message=FALSE)

library('vtreat')
library('ggplot2')
library('tidyr')

haveWVPlots <- require('WVPlots',quietly=TRUE) # https://github.com/WinVector/WVPlots

barbell_plot = function(frame, xvar, ymin, ymax, colorvar=NULL) {
  if(is.null(colorvar)) {
    gplot = ggplot(frame, aes_string(x=xvar))
  } else {
    gplot = ggplot(frame, aes_string(x=xvar, color=colorvar))
  }
  
  gplot + geom_point(aes_string(y=ymin)) + 
    geom_point(aes_string(y=ymax)) +
    geom_linerange(aes_string(ymin=ymin, ymax=ymax))
}

dotplot_identity = function(frame, xvar, yvar, colorvar=NULL) {
  if(is.null(colorvar)) {
    gplot = ggplot(frame, aes_string(x=xvar, y=yvar, ymax=yvar))
  } else {
    gplot = ggplot(frame, aes_string(x=xvar, y=yvar, ymax=yvar, color=colorvar))
  }
  
  gplot + geom_point() + geom_linerange(ymin=0)
}

extractProjection <- function(ndim,princ) {
  # pull off the rotation.  
  proj <- princ$rotation[,1:ndim] 
  # sign was arbitrary, so flip in convenient form
  for(i in seq_len(ndim)) {
    si <- sign(mean(proj[,i]))
    if(si!=0) {
      proj[,i] <- proj[,i]*si
    }
  }
  proj
}
```

First, let's build our example. In this data set, there are two (unobservable) processes: one that produces the output `yA` and one that produces the output `yB`.  We only observe the mixture of the two: `y =  yA + yB + eps`, where `eps` is a noise term.  

We'll set things up so that odd variables correspond to one process and even variable correspond to the other.

```{r makedata}

# build example where even and odd variables are bringing in noisy images
# of two different signals.
set.seed(23525)
n <- 500
yA <- rnorm(n)
yB <- rnorm(n)
d <- data.frame(y=yA+yB+rnorm(n))
yS <- list(yA,yB)
for(i in 1:5) {
  vi <- yS[[1+(i%%2)]] + rnorm(nrow(d))
  d[[paste('x',formatC(i,width=2,flag=0),sep='.')]] <- (1+i)*vi
}
```

Think of `y` as measuring some notion of success and the `x` variables as noisy estimates of two different factors that can each drive success.

Now we'll add lots of pure noise variables. Notice that variables are scaled by their order coming in, so that the later variables have larger magnitudes.  We build the noise by combining pairs of normals, to give a variable to variable correlation pattern.  As we will show, this correlation can undesirably outcompete the _y_ induced correlation among signaling variables *unless* we use _y_-aware scaling.

```{r addnoise}
vLast <- 0
for(i in 1:45) {
  vi <- rnorm(n)
  d[[paste('noise',formatC(i,width=2,flag=0),sep='.')]] <- (1+i)*(vi+vLast)
  vLast <- vi
}
summary(d[, c("y", "x.01", "x.02", "noise.01", "noise.20")])

```

Next, we'll design a treatment plan for the frame. 
```{r treatment}
# design treatment plan
treatmentsN <- designTreatmentsN(d,setdiff(colnames(d),'y'),'y',
                                 verbose=FALSE)

scoreFrame = treatmentsN$scoreFrame
scoreFrame$vartype = ifelse(grepl("noise", scoreFrame$varName), "noise", "signal")

dotplot_identity(scoreFrame, "varName", "sig", "vartype") + 
  coord_flip()  + ggtitle("vtreat variable signifcance estimates")+ 
  scale_color_manual(values = c("noise" = "#d95f02", "signal" = "#1b9e77")) 
```

Note that the noise variables have large significance values, denoting statistical insignificance.  `vtreat` uses a simulated "out of sample" estimate of signficance which is pessimal on noise variables.

## *Y*-Aware PCA

### Prepare the frame with *y*-aware scaling

Now let's prepare the treated frame, with scaling turned on. We will deliberately turn off variable 
pruning by setting `pruneSig = 1`. In real applications, you would want to set `pruneSig` to a value less than one to prune insignificant variables. However, here we turn off variable pruning to show that you can recover some of pruning's benefits via scaling effects, because the scaled noise variables should not have a major effect in the principal components analysis.


```{r prepscaled}
examplePruneSig = 1.0 
# prepare the treated frame, with scaling
dTrainNTreatedYScaled <- prepare(treatmentsN,d,pruneSig=examplePruneSig,scale=TRUE)

# get the variable ranges
ranges = vapply(dTrainNTreatedYScaled, FUN=function(col) c(min(col), max(col)), numeric(2))
rownames(ranges) = c("vmin", "vmax") 
rframe = as.data.frame(t(ranges))  # make ymin/ymax the columns
rframe$varName = rownames(rframe)
varnames = setdiff(rownames(rframe), "y")
rframe = rframe[varnames,]
rframe$vartype = ifelse(grepl("noise", rframe$varName), "noise", "signal")

# show a few columns
summary(dTrainNTreatedYScaled[, c("y", "x.01_clean", "x.02_clean", "noise.01_clean", "noise.20_clean")])
barbell_plot(rframe, "varName", "vmin", "vmax", "vartype") +
  coord_flip() + ggtitle("y-scaled variables: ranges") + 
  scale_color_manual(values = c("noise" = "#d95f02", "signal" = "#1b9e77"))

```

Notice that after the *y*-aware rescaling, the signal carrying variables have larger ranges than the noise variables.

### The Principal Components Analysis

Now do the principal components analysis. Notice the magnitudes of the singular values fall off quickly after the first two to five values. 

```{r scaledpca}
vars <- setdiff(colnames(dTrainNTreatedYScaled),'y')
# prcomp defaults to scale. = FALSE, but we already scaled/centered in vtreat- which we don't want to lose.
dm <- as.matrix(dTrainNTreatedYScaled[,vars])
princ <- prcomp(dm, center = FALSE,scale. = FALSE)
dotplot_identity(frame = data.frame(pc=1:length(princ$sdev), 
                            magnitude=princ$sdev), 
                 xvar="pc",yvar="magnitude") +
  ggtitle("Y-Scaled variables: Magnitudes of singular values")
```

When we look at the variable loadings of the first five principal components, we see that we recover the even/odd loadings of the original signal variables. `PC1` has the odd variables, and `PC2` has the even variables. These two principal components carry most of the signal. The next three principal components complete the basis for the five original signal variables. The noise variables have very small loadings, compared to the signal variables.

```{r scaledvarload}
proj <- extractProjection(2,princ)
rot5 <- extractProjection(5,princ)
rotf = as.data.frame(rot5)
rotf$varName = rownames(rotf)
rotflong = gather(rotf, "PC", "loading", starts_with("PC"))
rotflong$vartype = ifelse(grepl("noise", rotflong$varName), "noise", "signal")

dotplot_identity(rotflong, "varName", "loading", "vartype") + 
  facet_wrap(~PC,nrow=1) + coord_flip() + 
  ggtitle("Y-Scaled Variable loadings, first five principal components") + 
  scale_color_manual(values = c("noise" = "#d95f02", "signal" = "#1b9e77"))
```

Now let's look at the projection of the data onto its first two principal components. Notice that y increases both as we move up and as we move right. We have recovered two features that correlate with an increase in y.

```{r scaledplot}
# apply projection
projected <- as.data.frame(dm %*% proj,
                      stringsAsFactors = FALSE)

# plot data sorted by principal components
projected$y <- dTrainNTreatedYScaled$y
if(!haveWVPlots) {
  ggplot(projected, aes(x=PC1, y=PC2, color=y)) + 
    geom_point(size=3) + 
    scale_color_gradient2(low="#1b9e77",mid="gray", high="#d95f02") +
    ggtitle("Y-Scaled Data projected to first two principal components")
} else {
  ScatterHistN(projected,'PC1','PC2','y',
               "Y-Scaled Data projected to first two principal components")
}
```

It can be difficult to visually quantify the projection quality, though plotting is a great way to diagnose.
So let's add a quantitative measure of how much useful signal was captured by the first two principal componets.

```{r quant1}
model <- lm(y~PC1+PC2,data=projected)
summary(model)
projected$estimate <- predict(model,newdata=projected)
if(haveWVPlots) {
  ScatterHist(projected,'estimate','y','Recovered model versus truth',smoothmethod='lm',annot_size=3)
}
```

We see that the first two principal components capture 50% of the variance in *y*.  If our end goal had been 
regression instead of analysis we would have likely chosen variable pruning plus L2 (or Ridge) regularization over PCA as our preperation technique.

## *X*-only PCA

Now let's analyze the same data without scaling it to *y*. Standard PCA does not preserve _y_ relations -- as it was not designed to do so.

### Prepare the training data with _x_-only scaling.

We deliberately mis-scaled the original data when we generated it.  Mis-scaled data is a common problem in data science situations, but perhaps less common in carefully curated scientific situations.  So for this problem the best
practice to try to re-scale the *x* variables. Standard practice is to center the data at mean zero and scale it to unit standard deviation.

```{r xonlyexample}
dTrainNTreatedUnscaled <- prepare(treatmentsN,d,pruneSig=examplePruneSig,scale=FALSE)

# scale the data
dTrainNTreatedXscaled <- as.data.frame(scale(dTrainNTreatedUnscaled[,colnames(dTrainNTreatedUnscaled)!='y'],
                               center=TRUE,scale=TRUE),stringsAsFactors = FALSE)
dTrainNTreatedXscaled$y <- dTrainNTreatedUnscaled$y

# get the variable ranges
ranges = vapply(dTrainNTreatedXscaled, FUN=function(col) c(min(col), max(col)), numeric(2))
rownames(ranges) = c("vmin", "vmax") 
rframe = as.data.frame(t(ranges))  # make ymin/ymax the columns
rframe$varName = rownames(rframe)
varnames = setdiff(rownames(rframe), "y")
rframe = rframe[varnames,]
rframe$vartype = ifelse(grepl("noise", rframe$varName), "noise", "signal")

summary(dTrainNTreatedXscaled[, c("y", "x.01_clean", "x.02_clean", "noise.01_clean", "noise.20_clean")])
barbell_plot(rframe, "varName", "vmin", "vmax", "vartype") +
  coord_flip() + ggtitle("x scaled variables: ranges") + 
  scale_color_manual(values = c("noise" = "#d95f02", "signal" = "#1b9e77"))

```

Note that now the signal and noise variables have commeasurate ranges.

### The principal components analysis

```{r xscaledPCA}
vars <- setdiff(colnames(dTrainNTreatedXscaled),'y')

dm <- as.matrix(dTrainNTreatedXscaled[,vars])
princ <- prcomp(dm,center = FALSE,scale. = FALSE) # scaled/centered during data prep, don't do it here.
dotplot_identity(frame = data.frame(pc=1:length(princ$sdev), 
                            magnitude=princ$sdev), 
                 xvar="pc",yvar="magnitude") +
  ggtitle("x scaled variables: Magnitudes of singular values")
```

This time, the magnitudes of the singular values do not fall off as quickly. 

Now let's look at the variable loadings of the first five principal components, as before.

```{r xscaledload}
proj <- extractProjection(2,princ)
rot5 <- extractProjection(5,princ)
rotf = as.data.frame(rot5)
rotf$varName = rownames(rotf)
rotflong = gather(rotf, "PC", "loading", starts_with("PC"))
rotflong$vartype = ifelse(grepl("noise", rotflong$varName), "noise", "signal")

dotplot_identity(rotflong, "varName", "loading", "vartype") + 
  facet_wrap(~PC,nrow=1) + coord_flip() + 
  ggtitle("x scaled variable loadings, first 5 principal components") + 
  scale_color_manual(values = c("noise" = "#d95f02", "signal" = "#1b9e77"))
```

The signal variables still show the odd loading, but the even loading is missing.  And the noise variables dominate the projection, in aggregate swamping out the contributions from the signal variables.  This makes sense, as the correlation among the noise variables could be, for all PCA knows, due to an unknown different process *y'* that is unrelated to the *y* we are trying to predict. But we might want to predict *y'* in the future! In other words, without the _y_ hint, PCA is forced to prepare for any possible prediction task, whereas _y_-aware processing can optimize for the actual task at hand.

Let's project the data onto the first two principal components.


```{r xscaledplot}
# apply projection
projected <- as.data.frame(dm %*% proj,
                      stringsAsFactors = FALSE)

# plot data sorted by principal components
projected$y <- dTrainNTreatedXscaled$y
if(!haveWVPlots) {
  ggplot(projected, aes(x=PC1, y=PC2, color=y)) + 
    geom_point(size=3) + 
    scale_color_gradient2(low="#1b9e77",mid="gray", high="#d95f02") +
    ggtitle("x scaled Data projected to first two principal components")
} else {
  ScatterHistN(projected,'PC1','PC2','y',
               "x scaled Data projected to first two principal components")
}
```

And we see that _y_ is not as well ordered by PC1 and PC2 here, as it was with the *y*-aware PCA.  We
again fit a linear model to quantify the quality of signal captured.

```{r quant2}
model <- lm(y~PC1+PC2,data=projected)
summary(model)
projected$estimate <- predict(model,newdata=projected)
if(haveWVPlots) {
  ScatterHist(projected,'estimate','y','Recovered model versus truth',smoothmethod='lm',annot_size=3)
}
```

This model only explains about 11% of the variance in *y*; the two processes that produced *y* have diffused amongst the principal components, rather than mostly concentrating in the first two, as in the *y*-aware PCA.

## Is this the same as _caret::preProcess_?

_caret::preProcess_ is largely designed to implement a number of sophisticated _x_ alone transformations, 
groupings, prunings, and repairs (see [caret/preprocess.html#all](http://topepo.github.io/caret/preprocess.html#all)) which
demonstrates "the function on all the columns except the last, which is the outcome" on 
the _schedulingData_ dataset.  So _caret::preProcess_ is a super-version of the _PCA_ step.

We could use it as follows either alone or _before_ vtreat design/prepare as a initial pre-processor.  Using it alone is similar to _PCA_ for this data set as the example doesn't have some of the additional problems
_caret::preProcess_ is designed to help with.

```{r caret}
library('caret')
origVars <- setdiff(colnames(d),'y')
# can try variations such adding/removing non-linear steps such as "YeoJohnson"
prep <- preProcess(d[,origVars],
                     method = c("center", "scale", "pca"))
prepared <- predict(prep,newdata=d[,origVars])
newVars <- colnames(prepared)
prepared$y <- d$y
print(length(newVars))
# The 35 PCA variables are designed to capture 95% of the in-sample explainable varation.
# This turns out to be about 50% of the variance.
modelB <- lm(paste('y',paste(newVars,collapse=' + '),sep=' ~ '),data=prepared)
print(summary(modelB)$r.squared)
print(summary(modelB)$adj.r.squared)
# The first 5 variables only capture about 16% of the in-sample variance as
# without being informed about y we can't know which variation to preserve and which we
# can ignore.
model5 <- lm(paste('y',paste(newVars[1:5],collapse=' + '),sep=' ~ '),data=prepared)
print(summary(model5)$r.squared)
print(summary(model5)$adj.r.squared)
```



## Other *Y*-aware Approaches to Dimensionality Reduction

If your goal is regression, there are other workable _y_-aware dimension reducing procedures, such as L2-regularized regression or partial least squares. Both methods are also related to principal components analysis (see Hastie, Tibshirani, and Friedman, *The Elements of Statistical Learning*, 2nd Edition, 2009).



